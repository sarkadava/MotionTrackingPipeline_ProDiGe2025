{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9952faf9",
   "metadata": {},
   "source": [
    "# Motion tracking with MediaPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb40c77",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fa5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following video(s) will be processed for masking: \n",
      "['c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_pr_36_p0_snijden_gebaren_video_raw_cam2.avi', 'c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_trial_34_p1_springen_combinatie_video_raw_cam2.avi', 'c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_trial_43_p0_sterk_gebaren_video_raw_cam2.avi', 'c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_trial_53_p1_vangen_gebaren_video_raw_cam2.avi']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "curfolder = os.getcwd()\n",
    "\n",
    "# This is where we store videos we want to track\n",
    "projectdata = os.path.join(curfolder, \"ToTrack\")\n",
    "if not os.path.exists(projectdata):\n",
    "    os.makedirs(projectdata)\n",
    "\n",
    "vfiles = glob.glob(os.path.join(projectdata, \"*.avi\"))  # Check the extension\n",
    "\n",
    "# Here we store our tracked videos with skeleton\n",
    "outputf_mask = os.path.join(curfolder, \"Output_Videos\")\n",
    "if not os.path.exists(outputf_mask):\n",
    "    os.makedirs(outputf_mask)\n",
    "\n",
    "# Here we store our time series data\n",
    "outputf_ts = os.path.join(curfolder, \"Output_TimeSeries\")\n",
    "if not os.path.exists(outputf_ts):\n",
    "    os.makedirs(outputf_ts)\n",
    "\n",
    "print(\"\\nThe following video(s) will be processed for masking: \")\n",
    "print(vfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add499",
   "metadata": {},
   "source": [
    "## Loading in Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169a31f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      "Note that we have the following number of pose keypoints for markers hands\n",
      "42\n",
      "\n",
      "Note that we have the following number of pose keypoints for markers face\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\nNote that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\nNote that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "\n",
    "# Functions that we need\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpositions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f48387",
   "metadata": {},
   "source": [
    "## Tracking - pose (in meters+pixels), hands (in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667bf5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_pr_36_p0_snijden_gebaren_video_raw_cam2.avi\n",
      "This is video number 0 of 4 videos in total\n",
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_trial_34_p1_springen_combinatie_video_raw_cam2.avi\n",
      "This is video number 1 of 4 videos in total\n",
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_trial_43_p0_sterk_gebaren_video_raw_cam2.avi\n",
      "This is video number 2 of 4 videos in total\n",
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_trial_53_p1_vangen_gebaren_video_raw_cam2.avi\n",
      "This is video number 3 of 4 videos in total\n"
     ]
    }
   ],
   "source": [
    "# We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf)) + \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    # Capture the video and check video settings\n",
    "    videoname = vidf.split(\"\\\\\")[-1]\n",
    "    # if os.path.isfile(outputf_mask + videoname):\n",
    "    #     print(\"The video file \" + videoname + \" already exists in the output folder. We will skip this video.\")\n",
    "    #     continue\n",
    "\n",
    "    #videoloc = projectdata + videoname\n",
    "    #print(vidf)\n",
    "    capture = cv2.VideoCapture(vidf)  # load the video capture\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)  # check frame width\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # check frame height\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)  # fps = frames per second\n",
    "\n",
    "    # Create an empty video file to project the pose tracking on\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # for different video formats you could use e.g., *'XVID', MP4V\n",
    "    out = cv2.VideoWriter(os.path.join(outputf_mask, videoname), fourcc, fps=samplerate, \n",
    "                          frameSize=(int(frameWidth), int(frameHeight)))\n",
    "    \n",
    "    # Initialize Mediapipe Holistic\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]  # These are the time series objects starting with column names initialized above\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    tsbody_world = [markerxyzbody]  # For world landmarks (3D coordinates)\n",
    "    tsface_world = [markerxyzface]  # For normalized 3D face landmarks\n",
    "    tshands_world = [markerxyzhands]  # For normalized 3D hand landmarks\n",
    "\n",
    "    with mp_holistic.Holistic(\n",
    "        model_complexity=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        static_image_mode=False,\n",
    "        enable_segmentation=True\n",
    "    ) as holistic:\n",
    "\n",
    "        while capture.isOpened():\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make holistic detection\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            ### Pose landmarks\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                samplebody = listpositions(results.pose_landmarks)\n",
    "\n",
    "                \n",
    "                # Save pose world landmarks (3D coordinates in meters)\n",
    "                if results.pose_world_landmarks:\n",
    "                    samplebody_world = listpositions(results.pose_world_landmarks)\n",
    "                    samplebody_world.insert(0, time)\n",
    "                    tsbody_world.append(samplebody_world)\n",
    "\n",
    "\n",
    "            else:\n",
    "                samplebody = [np.nan for _ in range(len(markerxyzbody)-1)]\n",
    "                samplebody.insert(0, time)\n",
    "                tsbody.append(samplebody)\n",
    "\n",
    "                # Append NaNs for world coordinates as well\n",
    "                samplebody_world = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                samplebody_world.insert(0, time)\n",
    "                tsbody_world.append(samplebody_world)\n",
    "\n",
    "            ### Hand landmarks\n",
    "            if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "                    \n",
    "                mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        \n",
    "                # Process hands separately\n",
    "                sampleLH = listpositions(results.left_hand_landmarks)\n",
    "                sampleRH = listpositions(results.right_hand_landmarks)\n",
    "\n",
    "                # Fill empty left hand with placeholders\n",
    "                if len(sampleLH) == 0:\n",
    "                    sampleLH = [\"\" for x in range(int(len(markerxyzhands)/2))]\n",
    "\n",
    "                # Combine hands\n",
    "                samplehands = sampleLH + sampleRH\n",
    "                samplehands.insert(0, time)\n",
    "                tshands.append(samplehands)\n",
    "\n",
    "            # Show and write output\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            out.write(image)\n",
    "            time += (1000 / samplerate)\n",
    "\n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:  # if there are no more frames, break the loop\n",
    "                break\n",
    "            \n",
    "    # Once done, de-initialize all processes\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "    # Save CSV data for body\n",
    "    filebody = open(os.path.join(outputf_ts, videoname + '_body.csv'), 'w+', newline='')\n",
    "    with filebody:\n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "\n",
    "     # Save world coordinates (in meters) to CSV for body, face, and hands\n",
    "    filebody_world = open(os.path.join(outputf_ts, videoname + '_body_world.csv'), 'w+', newline='')\n",
    "    with filebody_world:\n",
    "        write = csv.writer(filebody_world)\n",
    "        write.writerows(tsbody_world)\n",
    "\n",
    "    # Save CSV data for hands\n",
    "    filehands = open(os.path.join(outputf_ts, videoname + '_hands_hol.csv'), 'w+', newline='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d5506",
   "metadata": {},
   "source": [
    "## Tracking - hands (in meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caae396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf)) + \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    # Capture the video and check video settings\n",
    "    videoname = vidf.split(\"\\\\\")[-1]\n",
    "    if os.path.isfile(outputf_mask + videoname):\n",
    "        print(\"The video file \" + videoname + \" already exists in the output folder. We will skip this video.\")\n",
    "        continue\n",
    "\n",
    "    capture = cv2.VideoCapture(vidf)  # load the video capture\n",
    "\n",
    "    # Initialize Mediapipe Holistic\n",
    "    time = 0\n",
    "    tshands = [markerxyzhands]\n",
    "    tshands_world = [markerxyzhands]  # For normalized 3D hand landmarks\n",
    "\n",
    "\n",
    "    with mp_hands.Hands(\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "      \n",
    "      while capture.isOpened():\n",
    "        success, image = capture.read()\n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "          for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "            \n",
    "            samplehands = listpositions(hand_landmarks)\n",
    "            samplehands.insert(0, time)\n",
    "            tshands.append(samplehands)\n",
    "        \n",
    "        if results.multi_hand_world_landmarks:\n",
    "          for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
    "            samplehands_world = listpositions(hand_world_landmarks)\n",
    "            samplehands_world.insert(0, time)\n",
    "            tshands_world.append(samplehands_world)\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "          break\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_prodige",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
