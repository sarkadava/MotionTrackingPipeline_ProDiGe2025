{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fdf060d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part I: Motion tracking with MediaPipe\"\n",
    "authors: Šárka Kadavá (adapted from EnvisionBOX)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46c217",
   "metadata": {},
   "source": [
    "In this script, we will demonstrate how to use MediaPipe for markerless motion tracking performed over a video. We will use the MediaPipe Holistic model. This model provides full-body tracking including face, hands, and pose landmarks, we will however focus mainly on pose landmarks which can be extracted in real-world landmarks not only in pixels (i.e., 2D image coordinates).\n",
    "\n",
    "Unlike many other (better) algorithms, MediaPipe also provides third, depth dimension.\n",
    "\n",
    "After loading necessary components of the algorithm, we run single loop to capture motion over videos in a folder. In the following script, we will work with the results further to prepare them for an analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb40c77",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23fa5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following video(s) will be processed for masking: \n",
      "['c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_pr_36_p0_snijden_gebaren_video_raw_cam2.avi', 'c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_trial_34_p1_springen_combinatie_video_raw_cam2.avi', 'c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_trial_43_p0_sterk_gebaren_video_raw_cam2.avi', 'c:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\MotionTrackingPipeline_ProDiGe2025\\\\ToTrack\\\\0_1_trial_53_p1_vangen_gebaren_video_raw_cam2.avi']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "curfolder = os.getcwd()\n",
    "\n",
    "# This is where we store videos we want to track\n",
    "projectdata = os.path.join(curfolder, \"ToTrack\")\n",
    "if not os.path.exists(projectdata):\n",
    "    os.makedirs(projectdata)\n",
    "\n",
    "vfiles = glob.glob(os.path.join(projectdata, \"*.avi\"))  # Check the extension\n",
    "\n",
    "# Here we store our tracked videos with skeleton\n",
    "outputf_mask = os.path.join(curfolder, \"Output_Videos\")\n",
    "if not os.path.exists(outputf_mask):\n",
    "    os.makedirs(outputf_mask)\n",
    "\n",
    "# Here we store our time series data\n",
    "outputf_ts = os.path.join(curfolder, \"Output_TimeSeries\")\n",
    "if not os.path.exists(outputf_ts):\n",
    "    os.makedirs(outputf_ts)\n",
    "\n",
    "print(\"\\nThe following video(s) will be processed for masking: \")\n",
    "print(vfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add499",
   "metadata": {},
   "source": [
    "## Loading in Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a31f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      "Note that we have the following number of pose keypoints for markers hands\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "### MEDIAPIPE ###\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\nNote that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "# set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: # for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "\n",
    "\n",
    "### FUNCTIONS ###\n",
    "\n",
    "# check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "# take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) # ignore last element as this has nothing\n",
    "\n",
    "# make the stringifyd position traces into clean numerical values\n",
    "def listpositions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() # remove spaces in the string if present\n",
    "            tracking_p.append(stripped) # add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f48387",
   "metadata": {},
   "source": [
    "## Tracking - pose (in meters+pixels), hands (in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667bf5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_pr_36_p0_snijden_gebaren_video_raw_cam2.avi\n",
      "This is video number 0 of 4 videos in total\n",
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_trial_34_p1_springen_combinatie_video_raw_cam2.avi\n",
      "This is video number 1 of 4 videos in total\n",
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_trial_43_p0_sterk_gebaren_video_raw_cam2.avi\n",
      "This is video number 2 of 4 videos in total\n",
      "We will now process video:\n",
      "c:\\Users\\kadava\\Documents\\Github\\MotionTrackingPipeline_ProDiGe2025\\ToTrack\\0_1_trial_53_p1_vangen_gebaren_video_raw_cam2.avi\n",
      "This is video number 3 of 4 videos in total\n"
     ]
    }
   ],
   "source": [
    "# LOOP\n",
    "## We will now loop over all the videos that are present in the video file\n",
    "for vidf in vfiles:\n",
    "    print(\"We will now process video:\")\n",
    "    print(vidf)\n",
    "    print(\"This is video number \" + str(vfiles.index(vidf)) + \" of \" + str(len(vfiles)) + \" videos in total\")\n",
    "    \n",
    "    # RAW VIDEO\n",
    "    ## Capture the video and check video settings\n",
    "    videoname = vidf.split(os.sep)[-1]\n",
    "\n",
    "    # This can be useful if you for some reason rerun the code but do not want to reprocess videos that are already done\n",
    "    # if os.path.isfile(outputf_mask + videoname):\n",
    "    #     print(\"The video file \" + videoname + \" already exists in the output folder. We will skip this video.\")\n",
    "    #     continue\n",
    "\n",
    "    ## Get video properties\n",
    "    capture = cv2.VideoCapture(vidf)  # load the video capture\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)  # check frame width\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # check frame height\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)  # fps = frames per second\n",
    "\n",
    "    # Create an empty video file to project the pose tracking on\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # for different video formats you could use e.g., *'XVID', MP4V\n",
    "    out = cv2.VideoWriter(os.path.join(outputf_mask, videoname), fourcc, fps=samplerate, \n",
    "                          frameSize=(int(frameWidth), int(frameHeight)))\n",
    "    \n",
    "    # Initialize Mediapipe Holistic\n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]  # These are the time series objects starting with column names initialized above\n",
    "    tshands = [markerxyzhands]\n",
    "    tsbody_world = [markerxyzbody]  # For world landmarks (3D coordinates)\n",
    "\n",
    "    with mp_holistic.Holistic(\n",
    "        model_complexity=2,             # highest-quality pose model\n",
    "        min_detection_confidence=0.5,   # minimum confidence for the detection to be considered valid\n",
    "        min_tracking_confidence=0.5,    # minimum confidence for the tracking to be considered valid\n",
    "        static_image_mode=False,\n",
    "        enable_segmentation=True\n",
    "    ) as holistic:\n",
    "\n",
    "        ## Processing video frame-by-frame\n",
    "        while capture.isOpened():\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:                     # if there are no more frames, break the loop\n",
    "                break\n",
    "\n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make holistic detection\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Pose landmarks\n",
    "            if results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)    # Overlay the pose landmarks on the image\n",
    "                samplebody = listpositions(results.pose_landmarks)                  # Convert the pose landmarks to a list of positions\n",
    "\n",
    "                \n",
    "                # Pose world landmarks (3D coordinates in meters)\n",
    "                if results.pose_world_landmarks:\n",
    "                    samplebody_world = listpositions(results.pose_world_landmarks)\n",
    "                    samplebody_world.insert(0, time)\n",
    "                    tsbody_world.append(samplebody_world)\n",
    "\n",
    "\n",
    "            else:\n",
    "                samplebody = [np.nan for _ in range(len(markerxyzbody)-1)]\n",
    "                samplebody.insert(0, time)\n",
    "                tsbody.append(samplebody)\n",
    "\n",
    "                # Append NaNs for world coordinates as well\n",
    "                samplebody_world = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                samplebody_world.insert(0, time)\n",
    "                tsbody_world.append(samplebody_world)\n",
    "\n",
    "            # Hand landmarks\n",
    "            if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        \n",
    "                # Process hands separately\n",
    "                sampleLH = listpositions(results.left_hand_landmarks)\n",
    "                sampleRH = listpositions(results.right_hand_landmarks)\n",
    "\n",
    "                # Fill empty left hand with placeholders\n",
    "                if len(sampleLH) == 0:\n",
    "                    sampleLH = [\"\" for x in range(int(len(markerxyzhands)/2))]\n",
    "\n",
    "                # Combine hands\n",
    "                samplehands = sampleLH + sampleRH\n",
    "                samplehands.insert(0, time)\n",
    "                tshands.append(samplehands)\n",
    "\n",
    "            # Show and write output\n",
    "            cv2.imshow('Mediapipe Feed', image)\n",
    "            out.write(image)\n",
    "            time += (1000 / samplerate)\n",
    "\n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:  # if there are no more frames, break the loop\n",
    "                break\n",
    "            \n",
    "    # Once done, de-initialize all processes\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "    # Save CSV data for body\n",
    "    filebody = open(os.path.join(outputf_ts, videoname + '_body_px.csv'), 'w+', newline='')\n",
    "    with filebody:\n",
    "        write = csv.writer(filebody)\n",
    "        write.writerows(tsbody)\n",
    "\n",
    "    # Save world coordinates (in meters) to CSV for body, face, and hands\n",
    "    filebody_world = open(os.path.join(outputf_ts, videoname + '_body_world.csv'), 'w+', newline='')\n",
    "    with filebody_world:\n",
    "        write = csv.writer(filebody_world)\n",
    "        write.writerows(tsbody_world)\n",
    "\n",
    "    # Save CSV data for hands\n",
    "    filehands = open(os.path.join(outputf_ts, videoname + '_hands_px.csv'), 'w+', newline='')\n",
    "    with filehands:\n",
    "        write = csv.writer(filehands)\n",
    "        write.writerows(tshands)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt_prodige",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
